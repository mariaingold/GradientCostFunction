{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK6QyjQcC34K"
      },
      "source": [
        "## Calculating cost with gradient descent and learning rate\n",
        "- Change the iteration and learning rate vaules and see the impact on cost.\n",
        "- Low iteration values with high learning rate (i.e. big steps) may lead to miss the global minimum\n",
        "- Goal is to reach minimum cost with minimum iteration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code credit:codebasics https://codebasics.io/coming-soon\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "DGRx4cGADT3M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LxA8_z3sC34M",
        "outputId": "ac9b900c-7b28-4009-8779-f005ee07b32c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m 4.96, b 1.44, cost 89.0 iteration 0\n",
            "m 0.4991999999999983, b 0.26879999999999993, cost 71.10560000000002 iteration 1\n",
            "m 4.451584000000002, b 1.426176000000001, cost 56.8297702400001 iteration 2\n",
            "m 0.892231679999997, b 0.5012275199999995, cost 45.43965675929613 iteration 3\n",
            "m 4.041314713600002, b 1.432759910400001, cost 36.35088701894832 iteration 4\n",
            "m 1.2008760606719973, b 0.7036872622079998, cost 29.097483330142282 iteration 5\n",
            "m 3.7095643080294423, b 1.4546767911321612, cost 23.307872849944438 iteration 6\n",
            "m 1.4424862661541864, b 0.881337636696883, cost 18.685758762535738 iteration 7\n",
            "m 3.4406683721083144, b 1.4879302070713722, cost 14.994867596913156 iteration 8\n",
            "m 1.6308855378034224, b 1.0383405553279617, cost 12.046787238456794 iteration 9\n",
            "m 3.2221235247119777, b 1.5293810083298451, cost 9.691269350698109 iteration 10\n",
            "m 1.7770832372205707, b 1.1780607551353204, cost 7.8084968312098315 iteration 11\n",
            "m 3.0439475772474127, b 1.5765710804477953, cost 6.302918117062937 iteration 12\n",
            "m 1.8898457226770244, b 1.3032248704973899, cost 5.098330841763168 iteration 13\n",
            "m 2.898169312926714, b 1.6275829443328358, cost 4.133961682056365 iteration 14\n",
            "m 1.9761515088959358, b 1.4160484030347593, cost 3.361340532576948 iteration 15\n",
            "m 2.7784216197824048, b 1.6809279342791488, cost 2.741808050753047 iteration 16\n",
            "m 2.0415541605113807, b 1.5183370872989306, cost 2.244528230107478 iteration 17\n",
            "m 2.6796170361078637, b 1.735457156285639, cost 1.8449036666988363 iteration 18\n",
            "m 2.090471617540917, b 1.611567833948162, cost 1.5233119201782324 iteration 19\n",
            "m 2.5976890103737853, b 1.790290604096816, cost 1.2640979056612756 iteration 20\n",
            "m 2.1264168621494517, b 1.6969533824619085, cost 1.0547704368105268 iteration 21\n",
            "m 2.529385561184701, b 1.8447607474362664, cost 0.8853615531285766 iteration 22\n",
            "m 2.1521818147302194, b 1.7754939584778073, cost 0.7479156468369821 iteration 23\n",
            "m 2.472104720735685, b 1.8983676540508527, cost 0.6360820885229722 iteration 24\n",
            "m 2.1699839382964696, b 1.8480185634495874, cost 0.5447903801652151 iteration 25\n",
            "m 2.423763296438881, b 1.950743302915348, cost 0.4699911136477278 iteration 26\n",
            "m 2.1815831093070837, b 1.9152179921582295, cost 0.4084494012702221 iteration 27\n",
            "m 2.3826922006906663, b 2.0016232209455125, cost 0.35758014655339476 iteration 28\n",
            "m 2.1883747814212473, b 1.9776712492627107, cost 0.31531667795040486 iteration 29\n",
            "m 2.3475529664737507, b 2.0508239542984783, cost 0.280005985849834 iteration 30\n",
            "m 2.19146424741668, b 2.0358666977033213, cost 0.2503251729489924 iteration 31\n",
            "m 2.317271157065729, b 2.0982251873107836, cost 0.2252148202231392 iteration 32\n",
            "m 2.19172583072087, b 2.0902190019495084, cost 0.2038258415569305 iteration 33\n",
            "m 2.2909832477163747, b 2.1437555628915694, cost 0.1854770944836773 iteration 34\n",
            "m 2.1898500615476015, b 2.1410827139250586, cost 0.16962156815305135 iteration 35\n",
            "m 2.2679942505397945, b 2.1873814501542004, cost 0.15581941113049289 iteration 36\n",
            "m 2.1863812735157397, b 2.188763177870427, cost 0.14371641365577967 iteration 37\n",
            "m 2.247743906750233, b 2.2290980581236037, cost 0.13302683968149862 iteration 38\n",
            "m 2.181747562970493, b 2.2335252935837153, cost 0.1235197278285518 iteration 39\n",
            "m 2.2297797112222417, b 2.268922416384484, cost 0.11500795886067308 iteration 40\n",
            "m 2.176284659606544, b 2.2756005683762908, cost 0.1073395295828815 iteration 41\n",
            "m 2.213735385878407, b 2.306887840824943, cost 0.10039058653754176 iteration 42\n",
            "m 2.170254943136438, b 2.315192801071317, cost 0.09405986334903649 iteration 43\n",
            "m 2.1993136987020745, b 2.3430395801944157, cost 0.08826423771269985 iteration 44\n",
            "m 2.1638625904931037, b 2.3524826719863134, cost 0.08293518155053264 iteration 45\n",
            "m 2.1862727486718105, b 2.3774314010318136, cost 0.07801592372722821 iteration 46\n",
            "m 2.1572656385141533, b 2.3876314575042543, cost 0.07345918129710392 iteration 47\n",
            "m 2.1744150151272015, b 2.4101229178167802, cost 0.06922534441882239 iteration 48\n",
            "m 2.150585587951272, b 2.4207840437050385, cost 0.06528102333197575 iteration 49\n",
            "m 2.1635786121786147, b 2.441177514495622, cost 0.06159788433500965 iteration 50\n",
            "m 2.1439150477863547, b 2.4520713783305874, cost 0.05815171649232756 iteration 51\n",
            "m 2.153630302083688, b 2.470660734860243, cost 0.054921682590988646 iteration 52\n",
            "m 2.137323817683481, b 2.4816124722824338, cost 0.05188971727120564 iteration 53\n",
            "m 2.1444599118649865, b 2.4986390442291735, cost 0.04904004275389065 iteration 54\n",
            "m 2.1308637257526066, b 2.509516039457312, cost 0.04635877856867898 iteration 55\n",
            "m 2.1359758694885094, b 2.525178884782891, cost 0.04383362645496491 iteration 56\n",
            "m 2.124572474492945, b 2.535881845863144, cost 0.04145361541183607 iteration 57\n",
            "m 2.128101633371053, b 2.5503459627684273, cost 0.03920889490609494 iteration 58\n",
            "m 2.118476696509155, b 2.5608018247073736, cost 0.03709056666678448 iteration 59\n",
            "m 2.120772834793503, b 2.5742047184297996, cost 0.03509054742420437 iteration 60\n",
            "m 2.112594380710634, b 2.5843610027801502, cost 0.03320145649050715 iteration 61\n",
            "m 2.1139349893254455, b 2.5968179395942217, cost 0.03141652330669783 iteration 62\n",
            "m 2.1069367971074353, b 2.606638274382932, cost 0.0297295110602709 iteration 63\n",
            "m 2.1075416624945413, b 2.618246487870094, cost 0.0281346532591438 iteration 64\n",
            "m 2.1015100223265035, b 2.6277070518134993, cost 0.02662660077102551 iteration 65\n",
            "m 2.101552998161378, b 2.6385491128066176, cost 0.025200377334927134 iteration 66\n",
            "m 2.096316147250177, b 2.6476358156400974, cost 0.023851341948628327 iteration 67\n",
            "m 2.095934536582619, b 2.657782334457597, cost 0.022575156852933542 iteration 68\n",
            "m 2.0913542316575633, b 2.6664885833847243, cost 0.021367760086655644 iteration 69\n",
            "m 2.090656263915584, b 2.676000378847538, cost 0.020225341788425083 iteration 70\n",
            "m 2.0866210575773376, b 2.6843253115524512, cost 0.019144323582910155 iteration 71\n",
            "m 2.0856918466960472, b 2.693255154066937, cost 0.018121340518098442 iteration 72\n",
            "m 2.082111722558874, b 2.7012022430021245, cost 0.017153225123473996 iteration 73\n",
            "m 2.0810180142142363, b 2.709596257293525, cost 0.01623699324146153 iteration 74\n",
            "m 2.077820105696288, b 2.717172209303728, cost 0.015369831350564987 iteration 75\n",
            "m 2.0766140592050317, b 2.7250710050809133, cost 0.014549085151541019 iteration 76\n",
            "m 2.0737392325653374, b 2.732284895849552, cost 0.013772249230347736 iteration 77\n",
            "m 2.0724614332425584, b 2.7397244808822614, cost 0.013036957645638886 iteration 78\n",
            "m 2.0698615599121704, b 2.7465870759846718, cost 0.012340975315898037 iteration 79\n",
            "m 2.0685434179941087, b 2.7535995950692826, cost 0.011682190103287127 iteration 80\n",
            "m 2.066179196691222, b 2.760122819221025, cost 0.011058605508984853 iteration 81\n",
            "m 2.064844857288579, b 2.7667371537338745, cost 0.010468333909073289 iteration 82\n",
            "m 2.0626840746684203, b 2.7729336776379365, cost 0.009909590271584152 iteration 83\n",
            "m 2.061351937985791, b 2.7791759333750248, cost 0.009380686304666458 iteration 84\n",
            "m 2.0593680791107873, b 2.785058853801841, cost 0.00888002499345325 iteration 85\n",
            "m 2.0580520100509183, b 2.7909527592203687, cost 0.00840609548939642 iteration 86\n",
            "m 2.056223147935525, b 2.7965353529206687, cost 0.007957468320912865 iteration 87\n",
            "m 2.05493343816708, b 2.8021025854443096, cost 0.007532790898352296 iteration 88\n",
            "m 2.0532413459797505, b 2.8073981214530215, cost 0.007130783289727114 iteration 89\n",
            "m 2.0519854787579397, b 2.812658575950258, cost 0.0067502342464980354 iteration 90\n",
            "m 2.050414919687842, b 2.8176801739944057, cost 0.006389997461079277 iteration 91\n",
            "m 2.0491981775199255, b 2.8226521847051367, cost 0.006048988039717134 iteration 92\n",
            "m 2.047736336426391, b 2.8274127099427506, cost 0.005726179176078216 iteration 93\n",
            "m 2.0465622835434223, b 2.8321132348672426, cost 0.005420599012302664 iteration 94\n",
            "m 2.045198311770722, b 2.836625221187641, cost 0.005131327675503935 iteration 95\n",
            "m 2.0440691768841837, b 2.8410699961476715, cost 0.0048574944787420265 iteration 96\n",
            "m 2.042793827417138, b 2.845345591859636, cost 0.004598275276411798 iteration 97\n",
            "m 2.0417108070703494, b 2.8495492600018677, cost 0.004352889964781892 iteration 98\n",
            "m 2.0405161418256377, b 2.8536001910078013, cost 0.004120600119124239 iteration 99\n"
          ]
        }
      ],
      "source": [
        "def gradient_descent(x,y):\n",
        "    m_curr = b_curr = 0\n",
        "    iterations = 100       #change value\n",
        "    n = len(x)\n",
        "    learning_rate = 0.08   #change value\n",
        "\n",
        "    for i in range(iterations):\n",
        "        y_predicted = m_curr * x + b_curr\n",
        "        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\n",
        "        md = -(2/n)*sum(x*(y-y_predicted))\n",
        "        bd = -(2/n)*sum(y-y_predicted)\n",
        "        m_curr = m_curr - learning_rate * md\n",
        "        b_curr = b_curr - learning_rate * bd\n",
        "        print (\"m {}, b {}, cost {} iteration {}\".format(m_curr,b_curr,cost, i))\n",
        "\n",
        "x = np.array([1,2,3,4,5])\n",
        "y = np.array([5,7,9,11,13])\n",
        "\n",
        "gradient_descent(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MARIA\n",
        "- Iteration: 50 (cut it in half)\n",
        "- Learning rate: 0.16 (double learning rate)\n",
        "- Cost impact: Cost much higher. Lowest cost at 35th iteration, but still at 1.59. Then started going back up.\n"
      ],
      "metadata": {
        "id": "wphv-vKiDe7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7s4EJvsJC34M",
        "outputId": "acc62860-3907-4f85-a8c2-82ad88369c5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m 9.92, b 2.88, cost 89.0 iteration 0\n",
            "m -17.84320000000001, b -4.6848, cost 684.3024000000001 iteration 1\n",
            "m 59.38227200000004, b 16.823808000000014, cost 5305.280739840005 iteration 2\n",
            "m -155.87418112000017, b -42.686791680000034, cost 41170.52327071341 iteration 3\n",
            "m 443.70225643520047, b 123.49219553280017, cost 319530.69958830887 iteration 4\n",
            "m -1226.7621939281933, b -339.09947321548833, cost 2479958.1338217533 iteration 5\n",
            "m 3426.896222985916, b 949.9840643845334, cost 19247607.266573466 iteration 6\n",
            "m -9537.843183733661, b -2640.9512102849967, cost 149385767.21068403 iteration 7\n",
            "m 26580.597984882428, b 7363.3626333905195, cost 1159422445.2642632 iteration 8\n",
            "m -74042.01504995863, b -20507.40747478158, cost 8998584234.091995 iteration 9\n",
            "m 206282.90910168606, b 57138.177365108815, cost 69840392145.14615 iteration 10\n",
            "m -574675.6612067533, b -159174.75212934468, cost 542049754521.9056 iteration 11\n",
            "m 1601000.348285189, b 443452.6833105289, cost 4206991503822.6953 iteration 12\n",
            "m -4460225.533656784, b -1235409.6297026218, cost 32651573708125.625 iteration 13\n",
            "m 12425771.509329613, b 3441740.8441127297, cost 253417499096081.4 iteration 14\n",
            "m -34617005.49385886, b -9588353.994959775, cost 1966840233251329.2 iteration 15\n",
            "m 96439683.59968573, b 26712247.43753187, cost 1.526516723168138e+16 iteration 16\n",
            "m -268671750.29123867, b -74417765.11817664, cost 1.1847700015063814e+17 iteration 17\n",
            "m 748493875.1673712, b 207320802.879229, cost 9.195313324548643e+17 iteration 18\n",
            "m -2085232526.2658348, b -577575971.3228006, cost 7.136725864861152e+18 iteration 19\n",
            "m 5809258908.579793, b 1609071567.595697, cost 5.539001692764853e+19 iteration 20\n",
            "m -16184041144.592953, b -4482719883.391527, cost 4.298965706881446e+20 iteration 21\n",
            "m 45087194782.35011, b 12488429980.982998, cost 3.336540982300667e+21 iteration 22\n",
            "m -125608623623.34596, b -34791574601.107666, cost 2.5895776997597017e+22 iteration 23\n",
            "m 349933643157.81525, b 96926007952.5389, cost 2.0098397408170828e+23 iteration 24\n",
            "m -974881748382.2119, b -270026612020.89618, cost 1.5598897782223398e+24 iteration 25\n",
            "m 2715927553473.1553, b 752268382275.5942, cost 1.2106717121701068e+25 iteration 26\n",
            "m -7566315081727.001, b -2095747951383.9453, cost 9.396343351382486e+25 iteration 27\n",
            "m 21079032039290.555, b 5838553871519.72, cost 7.2927505854423e+26 iteration 28\n",
            "m -58724172455661.23, b -16265654125082.64, cost 5.66009660488237e+27 iteration 29\n",
            "m 163599942548355.6, b 45314560752381.46, cost 4.392950670841834e+28 iteration 30\n",
            "m -455773833544132.4, b -126242043534799.12, cost 3.409485198504095e+29 iteration 31\n",
            "m 1269742422324631.0, b 351698290598706.56, cost 2.646191635152336e+30 iteration 32\n",
            "m -3537381263232819.0, b -979797887824522.5, cost 2.053779313376241e+31 iteration 33\n",
            "m 9854806755658260.0, b 2729623448982834.5, cost 1.5939924425803576e+32 iteration 34\n",
            "m -2.7454551535282324e+16, b -7604470540123600.0, cost 1.2371396919109163e+33 iteration 35\n",
            "m 7.648576158743013e+16, b 2.118532950658699e+16, cost 9.601768342287971e+33 iteration 36\n",
            "m -2.1308203552664742e+17, b -5.902030705945378e+16, cost 7.452186353875567e+34 iteration 37\n",
            "m 5.936262243042272e+17, b 1.6442494530515296e+17, cost 5.783838921451831e+35 iteration 38\n",
            "m -1.6537860327395994e+18, b -4.5807221252455405e+17, cost 4.4889903554147844e+36 iteration 39\n",
            "m 4.607290126527362e+18, b 1.276145486913319e+18, cost 3.484024137717296e+37 iteration 40\n",
            "m -1.283547078628574e+19, b -3.5552195903652106e+18, cost 2.7040432772494006e+38 iteration 41\n",
            "m 3.5758397188190667e+19, b 9.90450263338597e+18, cost 2.0986795028430383e+39 iteration 42\n",
            "m -9.961948344229103e+19, b -2.759299950996059e+19, cost 1.628840667126375e+40 iteration 43\n",
            "m 2.775303778041356e+20, b 7.68714644378262e+19, cost 1.2641863206318855e+41 iteration 44\n",
            "m -7.731731579267349e+20, b -2.1415656687424838e+20, cost 9.811684380967072e+41 iteration 45\n",
            "m 2.1539866621746505e+21, b 5.966197661351766e+20, cost 7.615107743263225e+42 iteration 46\n",
            "m -6.00080136416989e+21, b -1.6621257547157448e+21, cost 5.910286520630199e+43 iteration 47\n",
            "m 1.6717660162235237e+22, b 4.630523796396388e+21, cost 4.587129681368656e+44 iteration 48\n",
            "m -4.657380645337333e+22, b -1.2900197574196282e+22, cost 3.56019266413664e+45 iteration 49\n"
          ]
        }
      ],
      "source": [
        "def gradient_descent(x,y):\n",
        "    m_curr = b_curr = 0\n",
        "    iterations = 50       #change value\n",
        "    n = len(x)\n",
        "    learning_rate = 0.16   #change value\n",
        "\n",
        "    for i in range(iterations):\n",
        "        y_predicted = m_curr * x + b_curr\n",
        "        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\n",
        "        md = -(2/n)*sum(x*(y-y_predicted))\n",
        "        bd = -(2/n)*sum(y-y_predicted)\n",
        "        m_curr = m_curr - learning_rate * md\n",
        "        b_curr = b_curr - learning_rate * bd\n",
        "        print (\"m {}, b {}, cost {} iteration {}\".format(m_curr,b_curr,cost, i))\n",
        "\n",
        "x = np.array([1,2,3,4,5])\n",
        "y = np.array([5,7,9,11,13])\n",
        "\n",
        "gradient_descent(x,y)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}